{"cells":[{"metadata":{},"cell_type":"markdown","source":"\n## Reproducibility\nThis experiment is a part of a reproducibility experiment, where different software platforms are tested, in order to investigatehow well they can support reproducibility. The experiment itself is presented to display how the different requirements can be represented, which is discussed in a seperate document. We have also put up a kernel with a script file which serves the same purpose here: https://www.kaggle.com/zoukon/keras2/code. \n\n## Overview\nThe purpose of this experiment to categorize the MNIST dataset by using a convolutional neural network (CNN). MNIST is a set of 70,000 labeled images \nof handwritten numbers, centered over 28x28 pixels. The purpose of the experiment is to train the neural network to recognize handwritten images \non this form, and verify the accuracy through the test set. We expect that the network should be able to correctly assess more than 98% \nof the test set after training, similar to results we have gotten while running the experiment locally.\nIn order to make the CNN, I will be using Tensorflow and Keras, and basing myself on the example presented in \nthe tensorflow guide for CNNs. https://www.tensorflow.org/tutorials/estimators/cnn as well as the CNN example from \nhttps://towardsdatascience.com/a-simple-2d-cnn-for-mnist-digit-recognition-a998dbc1e79a .\n\n\n## CNN\nA CNN is a type of deep neural network commonly used in image recognition. Very little preprocessing is usually used compared to other image\nclassification algorithms. The network typically consists of multiple hidden layers of three different types, each having a different purpose.\n\n#### Convolutional layer: \nThis layer applies convolution to the input. \nFor each subregion, the layer performs a set of mathematical operations to produce a single value in the output feature map. \nConvolutional layers then typically apply a ReLU activation function to the output to introduce nonlinearities into the model.\n#### Pooling layer:\nThe pooling layers combine the output of neuron clusters at one layer into a single neuron in the next layer. This is handled by a pooling\nalgorithm such as max, min or average pooling. The main purpose of this layer is to reduce processing time by discarding a set of the values. \nThere is usually a pooling layer behind every convolutional layer in the network. \n#### Dense layer:\nThe dense layers or fully connected layer (FC) perform classification \non the feature results from extraction and downsampling by the previous layers,\nby connecting every neuron in one layer to every neutron in another layer. The principle of this \nlayer is similar to a traditional multi-layer perceptron neural network. The dense layers are typically at the end of the network.\n\n## Preprocessing\nCNNs traditionally use very little preprocessing of the data. \nIn this case we simply import the data as a numpy.ndarray,\nnormalize the values, and reshape the matrix to pass it into the network. \nWe also convert the class vectors to binary class matrices for the classification."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import keras\nimport pandas as pd\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, MaxPooling2D, Conv2D, Flatten","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"batch_size = 128\nnum_epoch = 5\n\na = np.load(\"../input/mnist.npz\")\nX_test = a['x_test']\ny_test = a['y_test']\n\nX_train = a['x_train']\ny_train = a['y_train']\n\nX_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\nX_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\ninput_shape = (28, 28, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f0c9c7b9770595624ac1430337e703edf6ecec36"},"cell_type":"code","source":"#more reshaping\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\nX_train /= 255\nX_test /= 255\nprint('X_train shape:', X_train.shape) #X_train shape: (60000, 28, 28, 1)\n\n#set number of categories\nnum_category = 10\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_category)\ny_test = keras.utils.to_categorical(y_test, num_category)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Method\nThe first thing we do is to download the data and prepare it, so that we can pass it into the network. The full Mnist dataset is\navailable through in the keras datasets. In the notebook in the Keras-tensorflow workspace, we downloaded the \n\nThis implementation is built using 2 convolutional layers both followed by a pooling layer. after the convolutions we performed\ndropout to improve the models convergence. We then flatten the data to pass it on to the dense layers. Here we have 2 layers,\nwith a dropout inbetween. The last dense layer uses softmax as its loss function, while all previous layers use rectified linear regression as their \nactivation function. \n\nThis leaves us with the following structure of the network: \n\n`Conv(relu) -> Pool(Max) -> Conv(relu) -> Pool(Max) -> Dropout(0.25) -> Dense(relu) -> Flatten -> Dropout(0.5) -> Dense(softmax)`\n\nFor the first convolution, I chose an output space of 32 output, kernel size of 3x3, stride of 1x1 and no padding. \nThe second convolution is identical, except it has an output space of 64. \nPooling layers are both identical, and use MaxPooling with a pool size of 2x2, stride of 2x2 and no padding.\n\nRough overview of the code layout: \n```\nImport Mnist dataset\nReshape arrays\nconvert class vectors to binary class matrices\nInitialize model\nConv(relu) -> Pool(Max) -> Conv(relu) -> Pool(Max) -> \nDropout(0.25) -> Dense(relu) -> Flatten -> Dropout(0.5) -> Dense(softmax)\nCompile model\nEvaluate model\nSave results to file\nSave model as HDF5\n```\n"},{"metadata":{"trusted":true,"_uuid":"3cf6386f55bb2c2b5bf6725103132f11145155be"},"cell_type":"code","source":"##model building\nmodel = Sequential()\n#convolutional layer with rectified linear unit activation\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n#32 convolution filters used each of size 3x3\n#choose the best features via pooling\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n#again\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\n#64 convolution filters used each of size 3x3\n#choose the best features via pooling\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n#randomly turn neurons on and off to improve convergence\nmodel.add(Dropout(0.25))\n#flatten since too many dimensions, we only want a classification output\nmodel.add(Flatten())\n#fully connected to get all relevant data\nmodel.add(Dense(128, activation='relu'))\n#one more dropout for convergence' sake  \nmodel.add(Dropout(0.5))\n#output a softmax to squash the matrix into output probabilities\nmodel.add(Dense(num_category, activation='softmax'))\n#We use adam as our optimizer\n#categorical ce since we have multiple classes (10) \nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=\"adam\",\n              metrics=['accuracy'])\n\n#model training\nmodel_log = model.fit(X_train, y_train, batch_size=batch_size, epochs=num_epoch,\n          verbose=1, validation_data=(X_test, y_test))\n\n#Print scores\nscore = model.evaluate(X_test, y_test, verbose=0)\nprint('Test loss:', score[0]) #Test loss: 0.0296396646054\nprint('Test accuracy:', score[1]) #Test accuracy: 0.9904\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"350cab1a086324ebbe6ec6b4d5a275ffcacfcc21"},"cell_type":"code","source":"#Save the classifications to a .csv file called results.csv\nY_predicted = model.predict(X_test)\npred_label = Y_predicted.argmax(axis = 1)\nimage_id = range(1,len(Y_predicted)+1)\ndf = {'ImageId':image_id,'Label':pred_label}\ndf = pd.DataFrame(df)\ndf.to_csv('results.csv',index = False)\n\n#Save the model as a HDF5 file called model.h5\nmodel.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}